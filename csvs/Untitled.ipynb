{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] place search_query\n",
      "ipykernel_launcher.py: error: the following arguments are required: search_query\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beatl\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from lxml import html\n",
    "import unicodecsv as csv\n",
    "import requests\n",
    "from time import sleep\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "url = \n",
    "def parse(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chrome/70.0.3538.77 Safari/537.36'}\n",
    "    success = False\n",
    "    \n",
    "    for retry in range(10):\n",
    "        response = requests.get(url, verify=False, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            success = True\n",
    "            break\n",
    "        else:\n",
    "            print(\"Response received: %s. Retrying : %s\"%(response.status_code, url))\n",
    "            success = False\n",
    "    \n",
    "    if success == False:\n",
    "        print(\"Failed to process the URL: \", url)\n",
    "    \n",
    "    parser = html.fromstring(response.text)\n",
    "    listing = parser.xpath(\"//li[@class='regular-search-result']\")\n",
    "    raw_json = parser.xpath(\"//script[contains(@data-hypernova-key,'yelp_main__SearchApp')]//text()\")\n",
    "    scraped_datas = []\n",
    "\n",
    "    # Case 1: Getting data from new UI\n",
    "    if raw_json:\n",
    "        print('Grabbing data from new UI')\n",
    "        cleaned_json = raw_json[0].replace('<!--', '').replace('-->', '').strip()\n",
    "        json_loaded = json.loads(cleaned_json)\n",
    "        search_results = json_loaded['searchPageProps']['searchResultsProps']['searchResults']\n",
    "        \n",
    "        for results in search_results:\n",
    "            result = results['searchResultBusiness']\n",
    "            is_ad = result.get('isAd')\n",
    "            price_range = result.get('priceRange')\n",
    "            position = result.get('ranking')\n",
    "            name = result.get('name')\n",
    "            ratings = result.get('rating')\n",
    "            reviews = result.get('reviewCount')\n",
    "            address = result.get('formattedAddress')\n",
    "            neighborhood = result.get('neighborhoods')\n",
    "            category_list = result.get('categories')\n",
    "            full_address = address+' '+''.join(neighborhood)\n",
    "            url = \"https://www.yelp.com\"+result.get('businessUrl')\n",
    "            \n",
    "            category = []\n",
    "            for categories in category_list:\n",
    "                category.append(categories['title'])\n",
    "            business_category = ','.join(category)\n",
    "\n",
    "            # Filtering out ads\n",
    "            if is_ad == False:\n",
    "                data = {\n",
    "                    'business_name': name,\n",
    "                    'rank': position,\n",
    "                    'review_count': reviews,\n",
    "                    'categories': business_category,\n",
    "                    'rating': ratings,\n",
    "                    'address': full_address,\n",
    "                    'price_range': price_range,\n",
    "                    'url': url\n",
    "                }\n",
    "                scraped_datas.append(data)\n",
    "        return scraped_datas\n",
    "\n",
    "    # Case 2: Getting data from OLD UI\n",
    "    if listing:\n",
    "        print('Grabbing data from OLD UI')\n",
    "\n",
    "        for results in listing:    \n",
    "            raw_position = results.xpath(\".//span[@class='indexed-biz-name']/text()\")\n",
    "            raw_name = results.xpath(\".//span[@class='indexed-biz-name']/a//text()\")\n",
    "            raw_ratings = results.xpath(\".//div[contains(@class,'rating-large')]//@title\")\n",
    "            raw_review_count = results.xpath(\".//span[contains(@class,'review-count')]//text()\")\n",
    "            raw_price_range = results.xpath(\".//span[contains(@class,'price-range')]//text()\")\n",
    "            category_list = results.xpath(\".//span[contains(@class,'category-str-list')]//a//text()\")\n",
    "            raw_address = results.xpath(\".//address//text()\")\n",
    "            is_reservation_available = results.xpath(\".//span[contains(@class,'reservation')]\")\n",
    "            is_accept_pickup = results.xpath(\".//span[contains(@class,'order')]\")\n",
    "            url = \"https://www.yelp.com\"+results.xpath(\".//span[@class='indexed-biz-name']/a/@href\")[0]\n",
    "\n",
    "            name = ''.join(raw_name).strip()\n",
    "            position = ''.join(raw_position).replace('.', '').strip()\n",
    "            cleaned_reviews = ''.join(raw_review_count).strip()\n",
    "            reviews =  re.sub(\"\\D+\", \"\", cleaned_reviews)\n",
    "            categories = ','.join(category_list)\n",
    "            cleaned_ratings = ''.join(raw_ratings).strip()\n",
    "            if raw_ratings:\n",
    "                ratings = re.findall(\"\\d+[.,]?\\d+\", cleaned_ratings)[0]\n",
    "            else:\n",
    "                ratings = 0\n",
    "            price_range = len(''.join(raw_price_range)) if raw_price_range else 0\n",
    "            address  = ' '.join(' '.join(raw_address).split())\n",
    "            reservation_available = True if is_reservation_available else False\n",
    "            accept_pickup = True if is_accept_pickup else False\n",
    "            data = {\n",
    "                    'business_name': name,\n",
    "                    'rank': position,\n",
    "                    'review_count': reviews,\n",
    "                    'categories': categories,\n",
    "                    'rating': ratings,\n",
    "                    'address': address,                    \n",
    "                    'price_range': price_range,\n",
    "                    'url': url\n",
    "            }\n",
    "            scraped_datas.append(data)\n",
    "        return scraped_datas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    argparser = argparse.ArgumentParser()\n",
    "    argparser.add_argument('place', help='Location/ Address/ zip code')\n",
    "    search_query_help = \"\"\"Available search queries are:\\n\n",
    "                            Restaurants,\\n\n",
    "                            Breakfast & Brunch,\\n\n",
    "                            Coffee & Tea,\\n\n",
    "                            Delivery,\n",
    "                            Reservations\"\"\"\n",
    "    argparser.add_argument('search_query', help=search_query_help)\n",
    "    args = argparser.parse_args()\n",
    "    place = args.place\n",
    "    search_query = args.search_query\n",
    "    yelp_url = \"https://www.yelp.com/search?find_desc=%s&find_loc=%s\" % (search_query,place)\n",
    "    print (\"Retrieving :\", yelp_url)\n",
    "    scraped_data = parse(yelp_url)\n",
    "    with open(\"scraped_yelp_results_for_%s.csv\" % (place), \"wb\") as fp:\n",
    "        fieldnames = ['rank', 'business_name', 'review_count', 'categories', 'rating', 'address', 'price_range', 'url']\n",
    "        writer = csv.DictWriter(fp, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
    "        writer.writeheader()\n",
    "        if scraped_data:\n",
    "            print (\"Writing data to output file\")  \n",
    "            for data in scraped_data:\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
